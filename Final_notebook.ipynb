{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fin_Samp1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKanoCCZcDHIM4wBg09uFE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dineshkumar-Anbalagan/Audio-Sentiment-Analysis/blob/main/Final_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5J-le6dwmPT",
        "outputId": "db639b2f-1c5f-48da-a3eb-9d6043c61f6f"
      },
      "source": [
        "!pip3 install pydub\n",
        "!pip3 install SpeechRecognition"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting SpeechRecognition\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 104kB/s \n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32U4w9iKD0wD"
      },
      "source": [
        "### **IMPORTING AUDIO BASED LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cySv4Lmqwp1G"
      },
      "source": [
        "import speech_recognition as sr \n",
        "import os \n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LypQtaNMD8QF"
      },
      "source": [
        "### **IMPORTING TEXT ANALYTICS AND DATA MANIPULATION LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCfW5Gojx9R5",
        "outputId": "b2c37467-76fe-4208-b011-55be954e9652"
      },
      "source": [
        "#Importing Data Manipulation Library:\n",
        "import pandas as pd\n",
        "\n",
        "#Importing Scientific computing library:\n",
        "import numpy as np\n",
        "\n",
        "#Importing Plotting libraries:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Importing Text Analysis Libraries:\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "!pip install textblob \n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download()\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as vader\n",
        "print('Libraries Imported')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> gutenberg\n",
            "    Downloading package gutenberg to /root/nltk_data...\n",
            "      Unzipping corpora/gutenberg.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> stopwords\n",
            "    Downloading package stopwords to /root/nltk_data...\n",
            "      Unzipping corpora/stopwords.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> vader_lexicon\n",
            "    Downloading package vader_lexicon to /root/nltk_data...\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n",
            "Libraries Imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVUAHLlhEUj5"
      },
      "source": [
        "### **IMPORTING ML AND DL LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ED6MHkhm0Zw"
      },
      "source": [
        "#train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Deep Learning Libraries\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBQBhpH_FT-6"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAU1Q4C9n3Il"
      },
      "source": [
        "data = pd.read_csv(\"Tweets.csv\")\n",
        "df = data[['airline_sentiment','text']]\n",
        "df['text'] = df['text'].str.replace('@VirginAmerica', '')\n",
        "df['text'] = df['text'].apply(lambda x: x.lower())\n",
        "df['text'] = df['text'].apply(lambda x: re.sub('[^a-zA-Z0-9\\s]', '',x))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4vDVPaBoERE"
      },
      "source": [
        "tokenizer  = Tokenizer(num_words=5000, split = \" \")\n",
        "tokenizer.fit_on_texts(df['text'].values)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(df['text'].values)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSHHMEmLoJVq"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(5000, 256, input_length = X.shape[1]))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(256, return_sequences=True, dropout = 0.3, recurrent_dropout=0.2))\n",
        "model.add(LSTM(256, dropout = 0.3, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzbIqXM-oPfc"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze473hl2oUFj",
        "outputId": "cecd653c-aab7-47d4-bf6c-1085b31aca6a"
      },
      "source": [
        "y = pd.get_dummies(df['airline_sentiment']).values\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 8\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size = batch_size, verbose = 2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "366/366 - 194s - loss: 0.6524 - accuracy: 0.7292\n",
            "Epoch 2/8\n",
            "366/366 - 190s - loss: 0.4360 - accuracy: 0.8303\n",
            "Epoch 3/8\n",
            "366/366 - 191s - loss: 0.3474 - accuracy: 0.8711\n",
            "Epoch 4/8\n",
            "366/366 - 191s - loss: 0.2866 - accuracy: 0.8916\n",
            "Epoch 5/8\n",
            "366/366 - 191s - loss: 0.2337 - accuracy: 0.9137\n",
            "Epoch 6/8\n",
            "366/366 - 191s - loss: 0.1948 - accuracy: 0.9285\n",
            "Epoch 7/8\n",
            "366/366 - 191s - loss: 0.1640 - accuracy: 0.9380\n",
            "Epoch 8/8\n",
            "366/366 - 190s - loss: 0.1422 - accuracy: 0.9483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8551e4c190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caIjz9xt_0E2"
      },
      "source": [
        "## **OVERALL INTEGRATED FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cR5UFZQykb9"
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    import string\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator).lower()\n",
        "\n",
        "#function to remove stopwords\n",
        "sw = stopwords.words('english')\n",
        "def stopwords(text):\n",
        "    text = [word for word in text.split() if word not in sw]\n",
        "    return \" \".join(text)\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "#a function to stem words\n",
        "def stemming(text):    \n",
        "    text = [stemmer.stem(word) for word in text.split()]\n",
        "    return \" \".join(text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioxgaX7I4zF0"
      },
      "source": [
        "def classify(audio):\n",
        "    \"\"\"\n",
        "    Splitting the large audio file into chunks\n",
        "    and apply speech recognition on each of these chunks\n",
        "    \"\"\"\n",
        "    r = sr.Recognizer()\n",
        "\n",
        "    # open the audio file using pydub\n",
        "    sound = AudioSegment.from_wav(audio)  \n",
        "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
        "    chunks = split_on_silence(sound,\n",
        "        min_silence_len = 500,\n",
        "        # adjust this per requirement\n",
        "        silence_thresh = sound.dBFS-14,\n",
        "        # keep the silence for 1 second, adjustable as well\n",
        "        keep_silence=500,\n",
        "    )\n",
        "    folder_name = \"audio-chunks\"\n",
        "    # create a directory to store the audio chunks\n",
        "    if not os.path.isdir(folder_name):\n",
        "        os.mkdir(folder_name)\n",
        "    whole_text = \"\"\n",
        "    # process each chunk \n",
        "    for i, audio_chunk in enumerate(chunks, start=1):\n",
        "        # export audio chunk and save it in\n",
        "        # the `folder_name` directory.\n",
        "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
        "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
        "        # recognize the chunk\n",
        "        with sr.AudioFile(chunk_filename) as source:\n",
        "            audio_listened = r.record(source)\n",
        "            # try converting it to text\n",
        "            try:\n",
        "                text = r.recognize_google(audio_listened)\n",
        "            except:\n",
        "                text = f\"{text.capitalize()}. \"\n",
        "                #print(chunk_filename, \":\", text)\n",
        "                whole_text += text\n",
        "\n",
        "    x = remove_punctuation(whole_text) #punctuation removal\n",
        "    data = stopwords(x) #removing stopwords\n",
        "    x1 = stemming(data)\n",
        "\n",
        "    X2 = tokenizer.texts_to_sequences(x1)\n",
        "    X2 = pad_sequences(X2)\n",
        "\n",
        "    predictions1 = model.predict(X2)\n",
        "\n",
        "    pos_count, neu_count, neg_count = 0, 0, 0\n",
        "    real_pos, real_neu, real_neg = 0, 0, 0\n",
        "    for i, prediction in enumerate(predictions1):\n",
        "      if np.argmax(prediction) == 2:\n",
        "        pos_count += 1\n",
        "      elif np.argmax(prediction) == 1:\n",
        "        neu_count += 1\n",
        "      else:\n",
        "        neg_count += 1\n",
        "    return {'Positive': pos_count, 'Neutral': neu_count, 'Negative': neg_count}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hph0fKoF5kMP",
        "outputId": "9f1e2b38-1415-4d97-b8e1-07a1452c742b"
      },
      "source": [
        "classify(\"test1.wav\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Negative': 1, 'Neutral': 4, 'Positive': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70TXoywOCw4y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}